# -*- coding: utf-8 -*-
"""Demo_TransNAS_TSAD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGndOhpYYP1KE0RS7rccKNGbqLltHxJP
"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive')

# # %cd /content/drive/My Drive/MOAA Evo Transformer Paper Code/TransNAS_TSAD-main
# !ls

# !pip uninstall -y dgl
# !pip install optuna
# !pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
# !pip install dgl -f https://data.dgl.ai/wheels/torch-2.3/repo.html
# !pip install SciencePlots
# !pip install transformers
# !pip install timeout_decorator
# !pip install torchdata==0.7.1

"""#Setting Up the Environment: Library Imports and Custom Module Integration"""

import pickle
import os
import numpy as np
import pandas as pd
import tqdm
from src.models import *
from src.training_helpers import *
from src.pot_evaluation import *
from src.plotting_utilities import *
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torch.nn as nn
from time import time
from pprint import pprint
import matplotlib.pyplot as plt
import os
import subprocess
import sys
os.environ['DGLBACKEND'] = 'pytorch'
import dgl

# To write power measurement
import csv

# SQLite database setup
import sqlite3
import json

# To save graphs
import optuna.visualization as vis

"""# Defining Paths: Data Storage, Preprocessed Datasets, and Model Checkpoints"
"""

DATA_PATH = './'  # Replace with your actual path
OUTPUT_FOLDER = os.path.join(DATA_PATH, 'Pre_processed_data')
CHECKPOINT_FOLDER = os.path.join(DATA_PATH, 'checkpoints')

# Name of file where the power readings are being stored
FILE_NAME = 'power_output.csv'
DB_NAME = 'results.db'

# Create a directory for the results graphs and database
# Note: you probably want to add a number or something to this directory if you do this multiple times
#results_dir = "our_results"
# os.makedirs(results_dir, exist_ok=True)
# db_path = os.path.join(results_dir, "results.db")


# Creates SQLite database and table
def create_db():
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS study_results (
            trial_number INTEGER PRIMARY KEY,
            F1_value REAL, 
            params REAL,
            GPU_usage REAL,
            GPU_avg REAL
        )
    """)
    conn.commit()
    conn.close()


# Saving trial values to SQLite database
def save_trial_to_db(trial_num, f1, param_count, wattage, wattage_avg):
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute('''
        INSERT INTO study_results(trial_number, F1_value, params, GPU_usage, GPU_avg)
        VALUES (?, ?, ?, ?, ?)
    ''', (trial_num, f1, param_count, wattage, wattage_avg))
    conn.commit()
    conn.close()

    

# Average the power readings we have so far
def get_avg_power(filename):
 
    with open(filename, 'r') as f:
        reader = list(csv.reader(f, delimiter=','))
        num_iters = len(reader)

    total=0
    num_skips=0

    for i in range(num_iters):
       
        if reader[i][0] != '#gpu' and reader[i][0] != '#Idx':
            total = total + float(reader[i][1])
        else:
            num_skips+=1

    avg_power = total / (num_iters-num_skips)

    return total, avg_power

# Get baseline power by reading average before training begins
# BASE_POWER = get_avg_power(FILE_NAME)

"""# Optimizing TransNAS_TSAD: Hyperparameter Tuning with Optuna on the Given (SMAP) Dataset (Results included with many best models havi f1>90)

## Overview:
This script focuses on the optimization of the training and architectural parameters of TransNAS_TSAD model, specifically tailored for anomaly detection within the given dataset, utilizing the Optuna NSGA-II algorithm. The process is designed to explore the best model configurations through extensive trials, aiming to enhance performance metrics such as the F1 score.

## About the Config Object:
The Config object centralizes crucial settings, including the target dataset and model specifics, facilitating seamless adjustments across different phases of the optimization and evaluation process. It's passed throughout to ensure consistency and adaptability in model training and testing settings.
"""

import time
import warnings
import optuna
from optuna.exceptions import TrialPruned
from timeout_decorator import TimeoutError
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import scienceplots 
plt.style.use(["science", "no-latex"])

# Suppress future warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

class Config:
    def __init__(self):
        self.dataset = 'MBA'  #         # 'dataset': Specifies the dataset for model optimization and evaluation.  Change according to the dataset you're working with (e.g., SMAP, SWaT).
        self.model = 'TransNAS_TSAD'  #  'model': Designates the model to be optimized. Here, 'TransNAS_TSAD'  refers to our dynamic transformer model tailored for time-series anomaly detection.
        self.retrain = True           # 'retrain': Indicates whether the model should undergo retraining.   # Setting this to True forces the model to train from scratch, ignoring any pre-trained weights.
        self.test = False             # 'test': Controls whether the script is in testing mode. When set to False,  # the script focuses on training the model. Set to True for evaluating the model's performance.
config = Config()





# Objective function for Optuna study
def objective(trial):

    # Load dataset
    print("output folder: ", OUTPUT_FOLDER)
    train_loader, test_loader, labels = load_dataset(OUTPUT_FOLDER,config.dataset,config)
    print(labels.shape[1])  # Print label shape for debugging
    trainD, testD = next(iter(train_loader)), next(iter(test_loader))
    trainO, testO = trainD, testD

    #---------------------------Objective #2 --> Number of parameters---------------------------

    # Hyperparameter suggestions
    lr = trial.suggest_float("lr", 0.0001, 0.009)
    dropout_rate = trial.suggest_float("dropout_rate", 0.1, 0.5)
    dim_feedforward = trial.suggest_int("dim_feedforward", 8, 128)
    batch = trial.suggest_int("batch", 16, 128, step=16)
    encoder_layers = trial.suggest_int("encoder_layers", 1, 3)
    decoder_layers = trial.suggest_int("decoder_layers", 1, 3)
    activation_function = trial.suggest_categorical("activation_function", ["relu", "leaky_relu", "sigmoid", "tanh"])

    # Time augmentation and noise parameters
    time_warping = trial.suggest_categorical("time_warping", [True, False])
    time_masking = trial.suggest_categorical("time_masking", [True, False])
    gaussian_noise = trial.suggest_float("gaussian_noise", 1e-4, 1e-1, log=True)
    use_linear_embedding = trial.suggest_categorical("use_linear_embedding", [True, False])

    # Conditional hyperparameter based on dataset
    if config.dataset in ['MSL', 'SMAP', 'SWaT', 'WADI', 'SMD', 'NAB', 'MBA', 'UCR']:
        phase_type = trial.suggest_categorical("phase_type", ["2phase", "iterative"])
    else:
        phase_type = trial.suggest_categorical("phase_type", ["1phase", "2phase", "iterative"])

    self_conditioning = trial.suggest_categorical("self_conditioning", [True, False])
    layer_norm = trial.suggest_categorical("layer_norm", [True, False])
    positional_encoding_type = trial.suggest_categorical("positional_encoding_type", ["sinusoidal", "fourier"])
    num_ffn_layers = trial.suggest_int("num_ffn_layers", 1, 3)

    # Compile hyperparameters
    params = {
        'lr': lr,
        'dropout_rate': dropout_rate,
        'dim_feedforward': dim_feedforward,
        'batch': batch,
        'encoder_layers': encoder_layers,
        'decoder_layers': decoder_layers,
        'attention_type': "scaled_dot_product",
        'positional_encoding_type': positional_encoding_type,
        'phase_type': phase_type,
        'gaussian_noise_std': gaussian_noise,
        'time_warping': time_warping,
        'time_masking': time_masking,
        'self_conditioning': self_conditioning,
        'layer_norm': layer_norm,
        'activation_function': activation_function,
        'use_linear_embedding': use_linear_embedding,
        'nhead': labels.shape[1],  # Dynamic based on labels
        'num_ffn_layers': num_ffn_layers
    }

    # Start tracking power
    proc = subprocess.Popen(
        [sys.executable, "track_power.py"],
         stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
        stdin=subprocess.DEVNULL,
        start_new_session=True   
    )
    print("Started subprocess")

    # Load model with suggested parameters
    print("\n LOADING MODEL \n")
    model, optimizer, scheduler, epoch, accuracy_list = load_model(config.model, labels.shape[1],config, **params)
    model.double()
    trainD, testD = convert_to_windows(trainD, model,config), convert_to_windows(testD, model,config)

    trial_timeout=40;     #Pass it to pot_eval () Set it according to the size of dataset (some trials get stuck for infinite time due to irregular parameters combination)

    # Training phase
    if not config.test:
        print(f'Training {config.model} on {config.dataset}')
        num_epochs = 5
        start = time.time()

        for e in tqdm(range(epoch+1, epoch+num_epochs+1)):
            lossT, lr = optimize_model(e, model, trainD, trainO, optimizer, scheduler, config)
            accuracy_list.append((lossT, lr))
            print(f"Epoch {e}, Loss: {lossT}, Learning Rate: {lr}")
            

        print('Training time: ' + "{:10.4f}".format(time.time() - start) + ' s')
       #save_model(model, optimizer, scheduler, e, accuracy_list,config)
        #plot_accuracies(accuracy_list, f'{config.model}_{config.dataset}')

    # Testing phase
    model.eval()  # Set model to evaluation mode
    print(f'Testing {config.model} on {config.dataset}')
    loss, y_pred = optimize_model(0, model, testD, testO, optimizer, scheduler,config, training=False)

    # Prepare dataframe for storing results
    #df = pd.DataFrame()

    # Initialize an empty list to collect result DataFrames
    result_list = []

    try:
        lossT, _ = optimize_model(0, model, trainD, trainO, optimizer, scheduler, config, training=False)
        for i in range(loss.shape[1]):
            lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]
            result, pred = pot_eval(config,trial_timeout,lt, l, ls)
            # Add the result DataFrame to the list
            if isinstance(result, dict):
              result = pd.DataFrame([result])  # Convert dict to DataFrame
            result_list.append(result)
        df = pd.concat(result_list, ignore_index=True)
    except TimeoutError:
        print(f"Trial timed out during score calculation.")
        return {'f1': 0.0, 'num_params': float('inf')}
    except Exception as e:
        print(f"An exception occurred: {e}")
        return {'f1': 0.0, 'num_params': float('inf')}


    #------------------------Objective #1 --> Accuracy------------------------

    # Finalize results
    lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)
    labelsFinal = (np.sum(labels, axis=1) >= 1).astype(int)
    num_params = sum(p.numel() for p in model.parameters())
    result, _ = pot_eval(config,trial_timeout,lossTfinal, lossFinal, labelsFinal)
    Result2 = result
    #Result2.update(hit_att(loss, labels))
    #Result2.update(ndcg(loss, labels))
    #print(df)
    pprint(Result2)


    #----------------------Objective #3 --> GPU Usage------------------------
    total, avg_power = get_avg_power(FILE_NAME) #- BASE_POWER
    #print("\n", "AVG POWER", avg_power, "\n")
    # proc.kill()
    # proc.wait(timeout=10)
    # # os.remove(FILE_NAME)
    # print("\nKilled process")
    proc.terminate()   # sends SIGTERM on Unix, or calls TerminateProcess on Windows

    try:
        proc.wait(timeout=5)
    except subprocess.TimeoutExpired:
        # if it didnâ€™t die gracefully, kill it hard
        proc.kill()
        proc.wait()


    save_trial_to_db(trial.number, result['f1'], num_params, total, avg_power)
    return result['f1'], avg_power


# Plotting Pareto fronts with MatplotLib
def plot_pareto_front(points, x_axis_title, y_axis_title, num):
    x_vals, y_vals = zip(*points)

    title=f"Pareto Front for {x_axis_title}, and , {y_axis_title}"
    plt.figure(num=num)
    plt.scatter(x_vals, y_vals)
    plt.xlabel(x_axis_title)
    plt.ylabel(y_axis_title)
    plt.title(title)
    plt.grid(True)
    #plt.show()
    plt.savefig(f"{title}.png")
    # plt.pause(0.001)


"""## Running Multi-Objective Optimization for TransNAS_TSAD: Balancing F1 Score and Model complexity

### Overview:
This script initiates a focused multi-objective optimization task utilizing the NSGA-II algorithm through Optuna, aimed at enhancing the TransNAS_TSAD model. The dual objectives are to achieve a higher F1 score, indicative of model performance, and to reduce the number of parameters, reflecting model simplicity and efficiency. Over the course of 100 trials, this process navigates the complex optimization landscape to find a harmonious balance between these goals. Following the optimization phase, the script generates insightful visualizations to dissect and understand the trade-offs between maximizing performance and minimizing complexity, providing a holistic view of the model's optimization journey.


"""

# import optuna
# from optuna.visualization import (
#     plot_pareto_front,
#     plot_optimization_history,
#     plot_param_importances,
#     plot_contour,
#     plot_parallel_coordinate,
#     plot_edf,
# )



def main():

    # Create database if it doesn't already exist 
    create_db()
    print("\nSucessfully created DB\n")

    print("\n")
    print("SAMPLER")
    print("\n")
    # Use NSGA-II sampler for multi-objective optimization
    sampler = optuna.samplers.NSGAIISampler()

    # Create a multi-objective study
    print("\n")
    print("create study")
    print("\n")
    # study = optuna.create_study(directions=["maximize", "minimize", "minimize"], sampler=sampler)
    study = optuna.create_study(directions=["maximize", "minimize"], sampler=sampler)
    print("\n")
    print("optimize")
    print("\n")
    study.optimize(objective, n_trials=10)

    pareto_trials = []
    graph1_points = []
    graph2_points = []
    graph3_points = []

    # Save trials to database
    # for trial in study.trials:
    #     print(trial)
    #     # save_trial_to_db(trial)
    #     pareto_trials.append(trial)

    print('Number of finished trials: ', len(study.trials))
    print('Best trials on the Pareto front:')

    # Iterate over the best trials (Pareto front)
    for trial in study.best_trials:
        print(f"Trial Number: {trial.number}")
        print(f"  Value for F1: {trial.values[0]}")
        # print(f"  Number of Parameters: {-trial.values[1]}")
        print("  Params: ")
        for key, value in trial.params.items():
            param_string = f"    {key}: {value}\n"
            print(param_string)
            with open("params.txt", "a") as f:
                f.write(param_string)
        print(f"  Value for GPU Usage: {trial.values[1]}")



    # Output average GPU usage 
    # avg_power = get_avg_power(FILE_NAME) - BASE_POWER
    # print("\n", "AVG POWER", avg_power, "\n")

    # Visualize the Pareto front
    # pareto_front = customize_pareto_front(study, target_names=["F1 Score", "Number of Parameters", "GPU Usage"])
    # pareto_front.show()
    # # pareto_front.write_image(os.path.join(results_dir, "pareto_front.png"))
    # #for trial in pareto_trials:
    #     # graph1_points.append(trial.values[0]) 
    #     # graph1_points.append(trial.values[1]) 
    #     # graph2_points.append(trial.values[1])
    #     # graph2_points.append(trial.values[2])  
    #     # graph3_points.append(trial.values[2])
    #     # graph3_points.append(trial.values[0])  
    # graph1_points = [(trial.values[0], trial.values[1]) for trial in pareto_trials]
    # graph2_points = [(trial.values[1], trial.values[2]) for trial in pareto_trials]
    # graph3_points = [(trial.values[2], trial.values[0]) for trial in pareto_trials]
    # plot_pareto_front(graph1_points, "F1 Score", "Number of Parameters", 1)
    # plot_pareto_front(graph2_points, "Number of Parameters", "GPU Usage (Watts)", 2)
    # plot_pareto_front(graph3_points, " Usage (Watts)", "F1 Score", 3)

    # # Visualize optimization history for the first objective (e.g., F1 score)
    # optimization_history_f1 = customize_optimization_history(study, target=lambda t: t.values[0], title='F1 Score Optimization History')
    # optimization_history_f1.show()

    # # Visualize optimization history for the second objective (e.g., Number of Parameters)
    # optimization_history_params = customize_optimization_history(study, target=lambda t: t.values[1], title='Parameter Count Optimization History')
    # optimization_history_params.show()

    # # Visualize optimization history for the third  objective (e.g., GPU Usage)
    # optimization_history_dropout = customize_optimization_history(study, target=lambda t: t.values[2], title='GPU Usage Optimization History')
    # optimization_history_dropout.show()

    # # Visualize parameter importances for the first objective
    # param_importances_f1 = customize_param_importances(study, target=lambda t: t.values[0], title='F1 Score Parameter Importances')
    # param_importances_f1.show()

    # # Visualize parameter importances for the second objective
    # param_importances_params = customize_param_importances(study, target=lambda t: t.values[1], title='Parameter Count Importances')
    # param_importances_params.show()

    # # Visualize parameter rate importances for the third objective
    # param_importances_dropout = customize_param_importances(study, target=lambda t: t.values[2], title='GPU Usage Importances')
    # param_importances_dropout.show()

    # # Visualize contour plot of selected hyperparameters
    # contour_plot = customize_contour_plot(study, params=["lr", "dropout_rate"], target=lambda t: t.values[0])
    # contour_plot.show()

    # # Visualize parallel coordinate
    # parallel_coordinate = customize_parallel_coordinate(study, target=lambda t: t.values[0])
    # parallel_coordinate.show()

    # # Visualize parallel coordinate
    # parallel_coordinate = customize_parallel_coordinate(study, target=lambda t: t.values[1])
    # parallel_coordinate.show()

    # # Visualize empirical distribution function (EDF)
    # edf_plot = customize_edf_plot(study, target=lambda t: t.values[0])
    # edf_plot.show()

if __name__ == '__main__':
    # Start tracking power by running the track_power.py script first in a separate terminal
    # Wait for some values to populate the CSV file then proceed here
    main()

"""# NSGA-II NAS Optimized TransNAS_TSAD Model Training/testing on the WADI Dataset

In this demonstration, we present the application and evaluation of a dynamic transformer model, TransNAS_TSAD, that was fine-tuned through a comprehensive NSGA-II (Non-dominated Sorting Genetic Algorithm II) NAS (Neural Architecture Search) process. This model underwent 100 trials of optimization specifically for the WADI dataset, during which the NSGA-II algorithm identified the most effective configuration for our purposes. The parameters showcased in this demo are directly derived from the best-performing model configuration obtained through this rigorous search process, ensuring we utilize a highly optimized setup for demonstrating the model's capabilities on the WADI dataset
"""

import torch
import time
import pandas as pd
import numpy as np
from tqdm import tqdm
# Additional necessary imports (such as for the functions mentioned above)
# Suppress future warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Import for reading in power 
import csv

class Config:
    def __init__(self):
        self.dataset = 'MBA'
        self.model = 'TransNAS_TSAD'
        self.retrain = True
        self.test = False

config = Config()


# # Run nvidia-smi and read in the gpu power 
# def get_power(filename, num_iters):
#     command = 'nvidia-smi dmon -s p -c ' + str(num_iters) + ' --format csv >' + filename
#     # print(command)
#     os.system(command)

#     # Get average GPU reading 
#     with open(filename, 'r') as f:
#         reader = list(csv.reader(f, delimiter=','))

#     sum=0
#     for i in range(2, num_iters+2):
#         sum = sum + float(reader[i][1])
#     print(sum)
#     avg_power = sum / num_iters

#     return avg_power

# Define static hyperparameters

def train_and_test_model(num_epochs=5):
    train_loader, test_loader, labels = load_dataset(OUTPUT_FOLDER,config.dataset,config)
    trainD, testD = next(iter(train_loader)), next(iter(test_loader))
    trainO, testO = trainD, testD

    static_params = {
        'lr':0.0015406367696774947,
        'dropout_rate': 0.28863840219073156,
        'dim_feedforward': 24,
        'batch': 128,
        'encoder_layers': 2,
        'decoder_layers': 1,
        'attention_type': 'scaled_dot_product',
        'positional_encoding_type': 'sinusoidal',
        'phase_type': '2phase',
        'gaussian_noise_std': 0.0010420486609326078,
        'time_warping': True,
        'time_masking': False,
        'self_conditioning': False,
        'layer_norm': False,
        'activation_function': 'leaky_relu',
        'use_linear_embedding': True,
        'nhead': labels.shape[1],  # Adjust based on your dataset
        'num_ffn_layers': 1
    }

#{'lr': 0.0015406367696774947, 'dropout_rate': 0.28863840219073156, 'dim_feedforward': 24, 'batch': 128, 'encoder_layers': 2, 'decoder_layers': 1, 'activation_function': 'leaky_relu', 'time_warping': True, 'time_masking': False, 'gaussian_noise': 0.0010420486609326078, 'use_linear_embedding': True, 'phase_type': '2phase', 'self_conditioning': False, 'layer_norm': False, 'positional_encoding_type': 'sinusoidal', 'num_ffn_layers': 1}.


    model, optimizer, scheduler, epoch, accuracy_list = load_model(config.model, labels.shape[1],config, **static_params)
    model.double()
    trainD, testD = convert_to_windows(trainD, model,config), convert_to_windows(testD, model,config)
    #if model.name in ['Attention', 'DAGMM', 'USAD', 'MSCRED', 'CAE_M', 'GDN', 'MTAD_GAT', 'MAD_GAN'] or 'TransNAS_TSAD' in model.name:
     #   trainD, testD = convert_to_windows(trainD, model), convert_to_windows(testD, model)

    # Training phase
    if not config.test:
        print(f'Training {config.model} on {config.dataset}')
        start = time.time()
        for e in tqdm(range(epoch+1, epoch+num_epochs+1)):
            lossT, lr = optimize_model(e, model, trainD, trainO, optimizer, scheduler,config)
            accuracy_list.append((lossT, lr))
        print('Training time: ' + "{:10.4f}".format(time.time() - start) + ' s')
        save_model(model, optimizer, scheduler, e, accuracy_list,config)
       #plot_accuracies(accuracy_list, f'{config.model}_{config.dataset}')

    # Testing phase
    torch.zero_grad = True
    model.eval()
    print(f'Testing {config.model} on {config.dataset}')
    loss, y_pred = optimize_model(0, model, testD, testO, optimizer, scheduler,config, training=False)

    # Plot curves
    if not config.test:
        if 'TransNAS_TSAD' in model.name:
            testO = torch.roll(testO, 1, 0)
        #plotter(f'{config.model}_{config.dataset}', testO, y_pred, loss, labels)
        #plotter_plotly(f'{config.model}_{config.dataset}', testO, y_pred, loss, labels)

    # Scores
    # df = pd.DataFrame()
    # lossT, _ = optimize_model(0, model, trainD, trainO, optimizer, scheduler,config, training=False)
    # for i in range(loss.shape[1]):
    #     lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]
    #     result, pred = pot_eval(config,100,lt, l, ls)
    #     df = df.append(result, ignore_index=True)

    df_list = []  # List to store DataFrames
    lossT, _ = optimize_model(0, model, trainD, trainO, optimizer, scheduler, config, training=False)

    for i in range(loss.shape[1]):
      lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]
      result, pred = pot_eval(config, 100, lt, l, ls)
      if isinstance(result, dict):
        result = pd.DataFrame([result])  # Convert dict to DataFrame
        df_list.append(result)

      # Add the result DataFrame to the list
      # df_list.append(result)

    # Concatenate all DataFrames in the list into a single DataFrame
    df = pd.concat(df_list, ignore_index=True)


    lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)
    labelsFinal = (np.sum(labels, axis=1) >= 1) + 0
    # Calculate the F1 score and the number of model parameters
    num_params = np.sum(p.numel() for p in model.parameters())
    result, _ = pot_eval(config,100,lossTfinal, lossFinal, labelsFinal)
    Result2= result
    #Result2.update(hit_att(loss, labels))
    #Result2.update(ndcg(loss, labels))
    #print(df)
    pprint(Result2)

# print("before")
# # Measure power before training
# print(get_power('output_before.csv', 10))
# print("after")
train_and_test_model()

"""# Evaluating a Pretrained WADI dataset Model on Test data
n this section, we focus on evaluating the performance of a pretrained TransNAS_TSAD model, which represents the pinnacle of optimization through our NSGA-II NAS (Non-dominated Sorting Genetic Algorithm II for Neural Architecture Search) process. This model, specifically fine-tuned for the WADI dataset, has been saved after rigorous training involving 100 trials to identify the most effective parameter configuration. The model achieved a remarkable F1 score of 82, underscoring the efficacy of the NSGA-II NAS approach in enhancing model performance. Here, we demonstrate the model's capabilities by loading the saved model and testing it on the WADI dataset to validate its achieved metrics and robustness in real-world scenarios.
"""

import torch
import time
import pandas as pd
import numpy as np
from tqdm import tqdm
# Additional necessary imports (such as for the functions mentioned above)

class Config:
    def __init__(self):
        self.dataset = 'MBA'
        self.model = 'TransNAS_TSAD'
        self.retrain = False
        self.test = True

config = Config()


# # Run nvidia-smi and read in the gpu power 
# def get_power(filename, num_iters):
#     command = 'nvidia-smi dmon -s p -c ' + str(num_iters) + ' --format csv >' + filename
#     # print(command)
#     os.system(command)

#     # Get average GPU reading 
#     with open(filename, 'r') as f:
#         reader = list(csv.reader(f, delimiter=','))

#     sum=0
#     for i in range(2, num_iters+2):
#         sum = sum + float(reader[i][1])
#     print(sum)
#     avg_power = sum / num_iters

#     return avg_power


# Define static hyperparameters

#def train_and_test_model(num_epochs=5):
#     train_loader, test_loader, labels = load_dataset(OUTPUT_FOLDER,config.dataset,config)
#     trainD, testD = next(iter(train_loader)), next(iter(test_loader))
#     trainO, testO = trainD, testD

#     static_params = {
#         'lr':0.0015406367696774947,
#         'dropout_rate': 0.28863840219073156,
#         'dim_feedforward': 24,
#         'batch': 128,
#         'encoder_layers': 2,
#         'decoder_layers': 1,
#         'attention_type': 'scaled_dot_product',
#         'positional_encoding_type': 'sinusoidal',
#         'phase_type': '2phase',
#         'gaussian_noise_std': 0.0010420486609326078,
#         'time_warping': True,
#         'time_masking': False,
#         'self_conditioning': False,
#         'layer_norm': False,
#         'activation_function': 'leaky_relu',
#         'use_linear_embedding': True,
#         'nhead': labels.shape[1],  # Adjust based on your dataset
#         'num_ffn_layers': 1
#     }

# #{'lr': 0.0015406367696774947, 'dropout_rate': 0.28863840219073156, 'dim_feedforward': 24, 'batch': 128, 'encoder_layers': 2, 'decoder_layers': 1, 'activation_function': 'leaky_relu', 'time_warping': True, 'time_masking': False, 'gaussian_noise': 0.0010420486609326078, 'use_linear_embedding': True, 'phase_type': '2phase', 'self_conditioning': False, 'layer_norm': False, 'positional_encoding_type': 'sinusoidal', 'num_ffn_layers': 1}.


#     model, optimizer, scheduler, epoch, accuracy_list = load_model(config.model, labels.shape[1],config, **static_params)
#     model.double()
#     trainD, testD = convert_to_windows(trainD, model,config), convert_to_windows(testD, model,config)
#     #if model.name in ['Attention', 'DAGMM', 'USAD', 'MSCRED', 'CAE_M', 'GDN', 'MTAD_GAT', 'MAD_GAN'] or 'TransNAS_TSAD' in model.name:
#      #   trainD, testD = convert_to_windows(trainD, model), convert_to_windows(testD, model)

#     # Training phase
#     if not config.test:
#         print(f'Training {config.model} on {config.dataset}')
#         start = time.time()
#         for e in tqdm(range(epoch+1, epoch+num_epochs+1)):
#             lossT, lr = optimize_model(e, model, trainD, trainO, optimizer, scheduler,config)
#             accuracy_list.append((lossT, lr))
#         print('Training time: ' + "{:10.4f}".format(time.time() - start) + ' s')
#         save_model(model, optimizer, scheduler, e, accuracy_list,config)
#        #plot_accuracies(accuracy_list, f'{config.model}_{config.dataset}')

#     # Testing phase
#     torch.zero_grad = True
#     model.eval()
#     print(f'Testing {config.model} on {config.dataset}')
#     loss, y_pred = optimize_model(0, model, testD, testO, optimizer, scheduler,config, training=False)

#     # Plot curves
#     if not config.test:
#         if 'TransNAS_TSAD' in model.name:
#             testO = torch.roll(testO, 1, 0)
#         #plotter(f'{config.model}_{config.dataset}', testO, y_pred, loss, labels)
#         #plotter_plotly(f'{config.model}_{config.dataset}', testO, y_pred, loss, labels)

#     # Scores
#     #df = pd.DataFrame()
#     # lossT, _ = optimize_model(0, model, trainD, trainO, optimizer, scheduler,config, training=False)
#     # for i in range(loss.shape[1]):
#     #     lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]
#     #     result, pred = pot_eval(config,100,lt, l, ls)
#     #     df = df.append(result, ignore_index=True)
#     df_list = []  # List to store DataFrames
#     lossT, _ = optimize_model(0, model, trainD, trainO, optimizer, scheduler, config, training=False)

#     for i in range(loss.shape[1]):
#       lt, l, ls = lossT[:, i], loss[:, i], labels[:, i]
#       result, pred = pot_eval(config, 100, lt, l, ls)
#       if isinstance(result, dict):
#         result = pd.DataFrame([result])  # Convert dict to DataFrame
#         df_list.append(result)

#     # Concatenate all DataFrames in the list into a single DataFrame
#     df = pd.concat(df_list, ignore_index=True)

#     lossTfinal, lossFinal = np.mean(lossT, axis=1), np.mean(loss, axis=1)
#     labelsFinal = (np.sum(labels, axis=1) >= 1) + 0
#     # Calculate the F1 score and the number of model parameters
#     num_params = sum(p.numel() for p in model.parameters())
#     result, _ = pot_eval(config,100,lossTfinal, lossFinal, labelsFinal)
#     Result2= result
#     #Result2.update(hit_att(loss, labels))
#     #Result2.update(ndcg(loss, labels))
#     #print(df)
#     pprint(Result2)

# print("before")
# # Measure power before training
# print(get_power('output_before.csv', 10))
# print("after")

#train_and_test_model()