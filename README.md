README updated 6/27/2025

  Project was inspired by the paper "TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural Architecture Search in Time Series Anomaly Detection" by Ijaz Ul Haq, Byung Suk Lee, and Donna M. Rizzo. The folder titled "TransNAS-TSAD" is from this work, with the modification of adding a third objective to their original multi-objective algorithm (MOOA) which was minimizing the amount of power draw from the GPU. Upon adding this to the original code base, we found that the relationship between GPU power draw, parameter count of the model, and the F1 score (representing the accuracy of anomaly detection) appeared to be uncorrelated. We also attempted to modify the MOOA to use CMA-ES and create a fitness function based on the accuracy values and GPU power draw readings, which provided similar results.

  In order to figure out the cause of this, we changed the task of the project to a standard image classification task using the CIFAR-10 dataset, the code for which is in the "ImageClassification folder". We handcrafted a base convolutional neural network (CNN) and changed the parameters of the model using configuration files. We then reported the GPU power draw of training that model, the parameter count, and the validation accuracy of the model and compared those. Once again, the results showed that there was no correlation among these metrics, which led us to hypothesize that the expected correlation (increased parameter count leading to increase accuracy with increased power draw) will only appear with larger scale models and larger scale hardware. Essentially, the known expected relationship among parameter count, accuracy of the task, and GPU power draw will only be seen when training with very high parameter count on stronger hardware than we have access to. So for now this work remains unpublished, but could be extended to prove this concept of there being a minimum level of parameter count and hardware to show an impact on energy usage. 

  To run the TransNAS-TSAD inspired code, set up the virtual environment in that folder and run the file "demo_transnas_tsad.py". The F1 score, parameter count, and GPU power draw (total and average) will be saved to a SQLite database called "results.db". From there you can graph the results using the functions in "view_results.py".

  To run the image classification task code with handcrafted neural networks, you can create a yaml configuration file in the folder "Handcrafted_NNs", and then change the path on line 27 of "cnn.py". In the yaml file you can change any of the parameters listed. Once you've chosen the parameters you want for your model, run "cnn.py" which creates 2 SQLite databases. During training one database will be created for the model that includes the epoch number, model name (from the configuration file), the test accuracy, the total wattage from that epoch, and the average wattage, this database is named the "*model name from config file*_results.db". After the model has been trained, all of the parameters from the configuration file, the final validation accuracy, the total wattage from training the entire model, and the model's parameter count are saved to the database "final_results.db". You can graph the values in these database by using and/or creating functions in "graph_results.py". Currently, the main function graphs a scatter plot of the model's parameter count, accuracy, and total wattage compared against each other in a 2D graph (3 graphs total), with each pointing being one model in "final_results.db". All previously run models (config files and databases) are saved in the folder "Handcrafted_NNs"
